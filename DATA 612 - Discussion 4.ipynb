{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA 612 - Discussion 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Charlie Rosemond 06.27.20\n",
    "\n",
    "Renee Diresta, Wired.com (2018): Up Next: A Better Recommendation System - https://www.wired.com/story/creating-ethical-recommendation-engines/\n",
    "\n",
    "In her 2018 article “Up Next: A Better Recommendation System,” Renee DiResta discusses how recommender systems—namely, recommenders deployed by social media sites like Pinterest and YouTube—can promote and exacerbate polarization and radicalization. Operating as intended, these systems recommend content based on users’ previous interactions with a site as well as interactions of similar users. Citing her use of Pinterest, DiResta notes she typically appreciates the recommended pins, but a recent project on disinformation—“one night of clicking through a Pinterest board of anti-Islamic memes”—spurred an overnight shift towards more radical content. Similar rapid transformations appear common across social media.\n",
    "\n",
    "DiResta cites several consequences of the prevalence of recommenders, including the amplification of conspiracy theories, the proliferation of misinformation, and others. Addressing these consequences is not straightforward, she explains, considering an already polarized conversation around curation and moderation of content and censorship. It is not surprising, then, that companies often adopt a hands-off approach to curtailing content-agnostic recommenders. Two years on from DiResta’s article, in response to the current climate, Twitter and Facebook recently started publicly flagging questionable content. Companies are increasingly implementing possible back-end solutions, like Google Jigsaw’s Project Redirect, in addition to more reactive moderation.\n",
    "\n",
    "I agree with DiResta’s push for prioritizing ethics over profit. Algorithms like recommenders and others shape our lives in numerous ways but are opaque in how they function. Companies must be held accountable for how their products operate and engage users. DiResta suggests creating “do not amplify” lists of topics or indicating the authenticity of content. The aforementioned efforts from Twitter and Facebook—most notably, Twitter’s response to misinformation tweeted by Donald Trump—represent recent attempts.\n",
    "\n",
    "There is broad evidence that thoughtful, intentional policy nudges promote better outcomes. One notable example is requiring individuals to opt out of automatic payroll contributions to retirement accounts. I can envision companies implementing nudges with respect to their recommender systems. Perhaps recommenders could be programmed, via a “do not amplify” list or the like, to default away from demonstrably radical content. Or they could offer a “balanced” set of recommendations. Of course, the question then becomes who makes a list or determines a balance. Companies are responsive to public sentiment, which increasingly appears to lean towards inclusivity."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
